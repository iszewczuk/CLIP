{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "943fa7f5",
   "metadata": {},
   "source": [
    "# Libs etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ff7f544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ftfy in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (6.1.1)\n",
      "Requirement already satisfied: regex in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (4.65.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (from ftfy) (0.2.5)\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /private/var/folders/_t/v7tb441j7_l32mrtl8d_nr7r0000gn/T/pip-req-build-myszvrr_\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /private/var/folders/_t/v7tb441j7_l32mrtl8d_nr7r0000gn/T/pip-req-build-myszvrr_\n",
      "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (from clip==1.0) (6.1.1)\n",
      "Requirement already satisfied: regex in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (from clip==1.0) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (from clip==1.0) (4.65.0)\n",
      "Requirement already satisfied: torch in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (from clip==1.0) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (from clip==1.0) (0.15.2)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.5)\n",
      "Requirement already satisfied: jinja2 in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.2)\n",
      "Requirement already satisfied: filelock in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (from torch->clip==1.0) (3.9.0)\n",
      "Requirement already satisfied: networkx in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (from torch->clip==1.0) (2.8.4)\n",
      "Requirement already satisfied: typing-extensions in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (from torch->clip==1.0) (4.4.0)\n",
      "Requirement already satisfied: sympy in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (from torch->clip==1.0) (1.11.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (from torchvision->clip==1.0) (9.4.0)\n",
      "Requirement already satisfied: requests in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (from torchvision->clip==1.0) (2.28.1)\n",
      "Requirement already satisfied: numpy in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.23.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages/mpmath-1.2.1-py3.10.egg (from sympy->torch->clip==1.0) (1.2.1)\n",
      "Requirement already satisfied: torch in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (2.0.1)\n",
      "Requirement already satisfied: torchvision in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (0.15.2)\n",
      "Requirement already satisfied: jinja2 in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: networkx in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: filelock in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: sympy in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: requests in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: numpy in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (from requests->torchvision) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages (from requests->torchvision) (1.26.14)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/ishevchuk/anaconda3/lib/python3.10/site-packages/mpmath-1.2.1-py3.10.egg (from sympy->torch) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install ftfy regex tqdm\n",
    "! pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install torch torchvision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f87cfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import os\n",
    "import torch\n",
    "import clip\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from clip import clip\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from torch.nn.functional import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b588659",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be73e5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa33dbb",
   "metadata": {},
   "source": [
    "# work with csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "951ca5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def create_full_path(row):\n",
    "#    return f\"remaster_fashion_for_clip/{row['file_path']}/{row['file_name']}.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbe679d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fashion_data['full_path'] = fashion_data.apply(create_full_path, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4a93609",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_data = pd.read_csv(\"remaster_fashion_for_clip/fashion.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "988d5162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.3</th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>file_name</th>\n",
       "      <th>description</th>\n",
       "      <th>file_path</th>\n",
       "      <th>category</th>\n",
       "      <th>full_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dress01</td>\n",
       "      <td>daisy slip dress</td>\n",
       "      <td>dress</td>\n",
       "      <td>dress</td>\n",
       "      <td>remaster_fashion_for_clip/dress/dress01.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>dress02</td>\n",
       "      <td>bunny tank top dress</td>\n",
       "      <td>dress</td>\n",
       "      <td>dress</td>\n",
       "      <td>remaster_fashion_for_clip/dress/dress02.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>dress03</td>\n",
       "      <td>floral midi slip dress</td>\n",
       "      <td>dress</td>\n",
       "      <td>dress</td>\n",
       "      <td>remaster_fashion_for_clip/dress/dress03.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>dress04</td>\n",
       "      <td>jeans slip dress</td>\n",
       "      <td>dress</td>\n",
       "      <td>dress</td>\n",
       "      <td>remaster_fashion_for_clip/dress/dress04.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>dress05</td>\n",
       "      <td>distressed double-corset dress</td>\n",
       "      <td>dress</td>\n",
       "      <td>dress</td>\n",
       "      <td>remaster_fashion_for_clip/dress/dress05.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.3  Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0 file_name  \\\n",
       "0             0             0             0           0   dress01   \n",
       "1             1             1             1           1   dress02   \n",
       "2             2             2             2           2   dress03   \n",
       "3             3             3             3           3   dress04   \n",
       "4             4             4             4           4   dress05   \n",
       "\n",
       "                      description file_path category  \\\n",
       "0                daisy slip dress     dress    dress   \n",
       "1            bunny tank top dress     dress    dress   \n",
       "2          floral midi slip dress     dress    dress   \n",
       "3                jeans slip dress     dress    dress   \n",
       "4  distressed double-corset dress     dress    dress   \n",
       "\n",
       "                                     full_path  \n",
       "0  remaster_fashion_for_clip/dress/dress01.jpg  \n",
       "1  remaster_fashion_for_clip/dress/dress02.jpg  \n",
       "2  remaster_fashion_for_clip/dress/dress03.jpg  \n",
       "3  remaster_fashion_for_clip/dress/dress04.jpg  \n",
       "4  remaster_fashion_for_clip/dress/dress05.jpg  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fashion_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c8e1f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_data = fashion_data.drop(fashion_data[fashion_data['file_name'] == 'ts03'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02b88bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_data = fashion_data.drop(columns=[\"Unnamed: 0.3\", \"Unnamed: 0.2\", \"Unnamed: 0.1\", \"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25808a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>description</th>\n",
       "      <th>file_path</th>\n",
       "      <th>category</th>\n",
       "      <th>full_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dress01</td>\n",
       "      <td>daisy slip dress</td>\n",
       "      <td>dress</td>\n",
       "      <td>dress</td>\n",
       "      <td>remaster_fashion_for_clip/dress/dress01.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dress02</td>\n",
       "      <td>bunny tank top dress</td>\n",
       "      <td>dress</td>\n",
       "      <td>dress</td>\n",
       "      <td>remaster_fashion_for_clip/dress/dress02.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dress03</td>\n",
       "      <td>floral midi slip dress</td>\n",
       "      <td>dress</td>\n",
       "      <td>dress</td>\n",
       "      <td>remaster_fashion_for_clip/dress/dress03.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dress04</td>\n",
       "      <td>jeans slip dress</td>\n",
       "      <td>dress</td>\n",
       "      <td>dress</td>\n",
       "      <td>remaster_fashion_for_clip/dress/dress04.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dress05</td>\n",
       "      <td>distressed double-corset dress</td>\n",
       "      <td>dress</td>\n",
       "      <td>dress</td>\n",
       "      <td>remaster_fashion_for_clip/dress/dress05.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  file_name                     description file_path category  \\\n",
       "0   dress01                daisy slip dress     dress    dress   \n",
       "1   dress02            bunny tank top dress     dress    dress   \n",
       "2   dress03          floral midi slip dress     dress    dress   \n",
       "3   dress04                jeans slip dress     dress    dress   \n",
       "4   dress05  distressed double-corset dress     dress    dress   \n",
       "\n",
       "                                     full_path  \n",
       "0  remaster_fashion_for_clip/dress/dress01.jpg  \n",
       "1  remaster_fashion_for_clip/dress/dress02.jpg  \n",
       "2  remaster_fashion_for_clip/dress/dress03.jpg  \n",
       "3  remaster_fashion_for_clip/dress/dress04.jpg  \n",
       "4  remaster_fashion_for_clip/dress/dress05.jpg  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fashion_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7715475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_text_features(row):\n",
    "    text = row['description']\n",
    "    text_tokens = clip.tokenize([text]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_tokens).float()\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    return text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fce0ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_image_features(row):\n",
    "    image_path = row['full_path']\n",
    "    img = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "\n",
    "    image_features = model.encode_image(img)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    return image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270e8470",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features_list = []\n",
    "image_features_list = []\n",
    "\n",
    "# Calculate features for each row in fashion_data and store them in the lists\n",
    "for index, row in fashion_data.iterrows():\n",
    "    text_features = calculate_text_features(row)\n",
    "    image_features = calculate_image_features(row)\n",
    "    text_features_list.append(text_features.detach().numpy())\n",
    "    image_features_list.append(image_features.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab87143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_data['text_features'] = text_features_list\n",
    "fashion_data['image_features'] = image_features_list\n",
    "\n",
    "# Set the display options for float formatting\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186ab91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fashion_data.to_csv(\"remaster_fashion_for_clip/f_w_F.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a989302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fashion_data = pd.read_csv(\"remaster_fashion_for_clip/f_w_f.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02042a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a3d52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = fashion_data['category'].value_counts()\n",
    "\n",
    "print(\"Number of items in each category:\")\n",
    "print(category_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070efb3d",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d197f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_image_info(dataset):\n",
    "    if dataset.empty:\n",
    "        raise ValueError(\"Dataset is empty.\")\n",
    "\n",
    "    random_index = np.random.randint(0, dataset.shape[0])\n",
    "    random_row = dataset.iloc[random_index]\n",
    "\n",
    "    random_image_info = {\n",
    "        \"file_path\": random_row[\"file_path\"],\n",
    "        \"full_path\": random_row[\"full_path\"],\n",
    "        \"file_name\": random_row[\"file_name\"],\n",
    "        \"description\": random_row[\"description\"],\n",
    "        \"text_features\": np.array(random_row[\"text_features\"]),\n",
    "        \"image_features\": np.array(random_row[\"image_features\"])\n",
    "    }\n",
    "\n",
    "    return random_image_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d6da9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_with_info(image_info):\n",
    "    image_path = image_info['full_path']\n",
    "    image = mpimg.imread(image_path)\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.title(\"Random Image\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"File name:\", image_info['file_name']+\".jpg\")\n",
    "    print(\"Directory Name:\", image_info['file_path'])\n",
    "    print(\"Description:\", image_info['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7300673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_images(image_info, needed_category, num_similar=3):\n",
    "    category_data = fashion_data[fashion_data['file_path'] == needed_category]\n",
    "\n",
    "    # Get the text and image features of the given image\n",
    "    query_text_features = torch.flatten(torch.tensor(image_info['text_features']))\n",
    "    query_img_features = torch.flatten(torch.tensor(image_info['image_features']))\n",
    "\n",
    "    similarities = []\n",
    "    for index, row in category_data.iterrows():\n",
    "        if row['file_name'] != image_info['file_name']:\n",
    "            # Flatten the 2D tensors from the dataset\n",
    "            row_text_features = torch.flatten(torch.tensor(row['text_features']))\n",
    "            row_img_features = torch.flatten(torch.tensor(row['image_features']))\n",
    "\n",
    "            similarity = (\n",
    "                torch.dot(row_text_features, query_text_features)\n",
    "                + torch.dot(row_img_features, query_img_features)\n",
    "            )\n",
    "            similarities.append((row['full_path'], similarity.item()))\n",
    "\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    similar_images = similarities[:num_similar]\n",
    "\n",
    "    return similar_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6f55f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similar_images(similar_images):\n",
    "    fig, axs = plt.subplots(1, len(similar_images), figsize=(15, 5))\n",
    "\n",
    "    for idx, (img_path, similarity) in enumerate(similar_images, 1):\n",
    "        img = Image.open(img_path)\n",
    "        axs[idx - 1].imshow(img)\n",
    "        axs[idx - 1].set_title(f\"Top {idx} Similar Image\\nSimilarity: {similarity:.2f}\")\n",
    "        axs[idx - 1].axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dac422",
   "metadata": {},
   "outputs": [],
   "source": [
    "needed_category1 = \"dress\"\n",
    "needed_category2 = \"top\"\n",
    "needed_category3 = \"pants\"\n",
    "needed_category4 = \"highheels\"\n",
    "needed_category5 = \"jacket\"\n",
    "needed_category6 = \"sneakers\"\n",
    "needed_category7 = \"blazer\"\n",
    "needed_category8 = \"tshirt\"\n",
    "needed_category9 = \"boots\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062cba71",
   "metadata": {},
   "source": [
    "# similar imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb346605",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_image_info = get_random_image_info(fashion_data)\n",
    "plot_image_with_info(random_image_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbeaad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_images1 = find_similar_images(random_image_info, needed_category1)\n",
    "plot_similar_images(similar_images1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e21c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_images2 = find_similar_images(random_image_info, needed_category2)\n",
    "plot_similar_images(similar_images2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38775898",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_images3 = find_similar_images(random_image_info, needed_category3)\n",
    "plot_similar_images(similar_images3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e9f427",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_images4 = find_similar_images(random_image_info, needed_category4)\n",
    "plot_similar_images(similar_images4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829e8173",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_images5 = find_similar_images(random_image_info, needed_category5)\n",
    "plot_similar_images(similar_images5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9aae40",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_images6 = find_similar_images(random_image_info, needed_category6)\n",
    "plot_similar_images(similar_images6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3b111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_images7 = find_similar_images(random_image_info, needed_category7)\n",
    "plot_similar_images(similar_images7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed44f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_images8 = find_similar_images(random_image_info, needed_category8)\n",
    "plot_similar_images(similar_images8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35886e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_images9 = find_similar_images(random_image_info, needed_category9)\n",
    "plot_similar_images(similar_images9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e15b4c6",
   "metadata": {},
   "source": [
    "# Let's try to search img by text description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b3742e",
   "metadata": {},
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenize and encode the text description using the same method you used in the dataset\n",
    "    text_tokens = clip.tokenize([text]).to(device)\n",
    "    return text_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6949e2fa",
   "metadata": {},
   "source": [
    "def calculate_text_features(text_tokens):\n",
    "    # Calculate text features using the CLIP model\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_tokens).float()\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    return text_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2742053d",
   "metadata": {},
   "source": [
    "def find_similar_images(text_query, k=5):\n",
    "    # Preprocess the text query\n",
    "    query_text_tokens = preprocess_text(text_query)\n",
    "\n",
    "    # Calculate text features for the query\n",
    "    query_text_features = calculate_text_features(query_text_tokens)\n",
    "\n",
    "    # Initialize a list to store image indices and their similarity scores\n",
    "    similarity_scores = []\n",
    "\n",
    "    for index, row in fashion_data.iterrows():\n",
    "        # Calculate the similarity between the query text features and each row's text features\n",
    "        row_text_features = torch.tensor(row['text_features'])\n",
    "        row_img_features = torch.tensor(row['image_features'])\n",
    "\n",
    "        similarity = torch.dot(query_text_features, row_text_features) + torch.dot(query_img_features, row_img_features)\n",
    "\n",
    "        # Append the image index and its similarity score to the list\n",
    "        similarity_scores.append((index, similarity.item()))\n",
    "\n",
    "    # Sort the images based on similarity scores (in descending order)\n",
    "    similarity_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the indices of the top-k most similar images\n",
    "    top_k_indices = [index for index, _ in similarity_scores[:k]]\n",
    "\n",
    "    # Return the top-k rows from the fashion_data DataFrame\n",
    "    return fashion_data.iloc[top_k_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa7f0be",
   "metadata": {},
   "source": [
    "text_query = \"red dress with floral pattern\"\n",
    "similar_images = find_similar_images(text_query, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aece6ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841424fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5023ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de96b38b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62174d14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
